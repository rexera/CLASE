\documentclass[10pt, a4paper]{article}

\usepackage[final]{lrec2026} % this is the new style
% the 'review' option anonymizes the paper following submission guideline
% the 'final' option produces the camera ready version (non anonymized)
% default version is 'final', so use review option for submission

\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{booktabs} % Added for \midrule
\usepackage{amsmath}
\usepackage{bookmark}
\usepackage{hyperref}
 
\title{CLASE: A Hybrid Method for Chinese Legalese Stylistic Evaluation}

\name{Yiran Rex Ma$^\dagger$\thanks{$^\dagger$Equal contribution.}\textsuperscript{\rm 1,2}, Yuxiao Ye$^\dagger$\textsuperscript{\rm 3}, Huiyuan Xie\textsuperscript{\rm 3}} 

\address{\textsuperscript{\rm 1}School of Foreign Languages, Peking University\\
\textsuperscript{\rm 2}Center for Digital Humanities, Peking University\\
\textsuperscript{\rm 3}Department of Computer Science and Technology, Tsinghua University\\
         Beijing, China \\
         \texttt{yiranrexma@outlook.com}\\}



\abstract{
  Legal text generated by large language models (LLMs) can usually achieve reasonable factual accuracy, but it frequently fails to adhere to the specialised stylistic norms and linguistic conventions of legal writing. In order to improve stylistic quality, a crucial first step is to establish a reliable evaluation method. However, having legal experts manually develop such a metric is impractical, as the implicit stylistic requirements in legal writing practice are difficult to formalise into explicit rubrics. Meanwhile, existing automatic evaluation methods also fall short: reference-based metrics conflate semantic accuracy with stylistic fidelity, and LLM-as-a-judge evaluations suffer from opacity and inconsistency. To address these challenges, we introduce \textit{\textbf{CLASE}} (\textbf{C}hinese \textbf{L}eg\textbf{A}lese \textbf{S}tylistic \textbf{E}valuation), a hybrid evaluation method that focuses on the stylistic performance of legal text. The method incorporates a hybrid scoring mechanism that combines 1) linguistic feature-based scores and 2) experience-guided LLM-as-a-judge scores. Both the feature coefficients and the LLM scoring experiences are learned from contrastive pairs of authentic legal documents and their LLM-restored counterparts. This hybrid design captures both surface-level features and implicit stylistic norms in a transparent, reference-free manner. Experiments on 200 Chinese legal documents show that CLASE achieves substantially higher alignment with human judgments than traditional metrics and pure LLM-as-a-judge methods. Beyond improved alignment, CLASE provides interpretable score breakdowns and suggestions for improvements, offering a scalable and practical solution for professional stylistic evaluation in legal text generation$^\star$\thanks{$^\star$Code and data for CLASE is available at: \url{https://github.com/rexera/CLASE}.}. 
 \\ \newline \Keywords{Legal text generation, Stylistic evaluation, Hybrid evaluation, LLM-as-a-judge} }

\begin{document}

\maketitleabstract

\section{Introduction}

Large Language Models (LLMs) have significantly advanced legal content generation, with advances on legal reasoning tasks and bar examinations \cite{bommarito2022gpt,katz2023gpt}. These developments have generated considerable interest in automating legal document production, ranging from contract drafting to judicial decision writing. However, while LLMs excel at generating factually accurate and logically coherent legal content, they consistently struggle with the specialized stylistic conventions (``legalese'') that characterize professional legal discourse \cite{tiersma1999legal,courtWritingGuide2010}.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{fig_intro.pdf}
    \caption{Comparison of Authentic Legal Writing and LLM-Generated Counterpart.}
    \label{fig:intro}
\end{figure}

Legal writing demands adherence to established stylistic norms that extend far beyond semantic correctness. Professional legal documents must conform to specific collocation patterns, formal register requirements, and domain-specific linguistic conventions that signal authority and credibility within the legal community and facing the general public \cite{foley2002language,li2020cohesion,lu2021legal,sun2017linguistic,li2022lexical}. 

LLM-generated legal documents exhibit two primary categories of stylistic deficiencies: \textit{insufficient sophistication}, failing to meet the expectations through inappropriate colloquialisms or non-standard term choices; and \textit{excessive stylistic elaboration}, where models artificially create a formal impression through verbose, unnecessarily complex, or archaic constructions. This overcompensation often results in hallucinated legal concepts. These dual deficiencies risk undermining professional acceptability (see Figure \ref{fig:intro}).

Current evaluation for legal text generation focuses on \textbf{reasoning capabilities} while neglecting \textbf{stylistic quality assessment} \cite{chalkidis2022lexglue,zhong2018cail2018,fu2022jec}. This assumes that semantic accuracy, represented by factual correctness, logical argumentation accuracy, and legal knowledge demonstration, constitutes the primary criterion for legal text quality, whereas stylistic appropriateness is just as important in legal competence.

\textbf{Stylistic quality} in legal writing encompasses multiple intricate dimensions that traditional evaluation approaches fail to capture. These include: 1) precise diction and lexical choice; 2) adherence to linguistic/legal conventions; 3) appropriate lexical collocation patterns; and 4) domain-specific common sense and world knowledge \cite{foley2002language}. These elements collectively contribute to what legal practitioners recognize as ``professional'' or ``sophisticated'' writing style, yet they remain largely underexplored in current automated evaluation systems. 

Traditional natural language generation (NLG) metrics could be inadequate for capturing stylistic nuances in legal text. Both branches of n-gram and embedding-based methods conflate semantic similarity with stylistic appropriateness \cite{mellish1998evaluation,papineni2002bleu,zhang2019bertscore}. These metrics may award high scores to texts that preserve semantic content while exhibiting stylistic violations that would be immediately apparent to legal professionals. Recent advances in LLM-as-a-Judge evaluation \cite{zheng2023judging,chan2023chateval} offer intuitive solutions through instruction understanding, but 1) suffer from limited interpretability \cite{wang2023survey}; 2) exhibit biases and consistency issues that undermine their reliability.

The challenge is compounded by the implicit nature of legal stylistic expertise. Writing appropriateness relies on tacit professional knowledge that resists explicit formalization \cite{tavakol2023importance}. Legal professionals develop stylistic intuition through years of practice and exposure to exemplary texts, making it difficult to translate this expertise into comprehensive evaluation rubrics. Manual annotation by legal experts is prohibitively time-consuming for large-scale evaluation needs.

To address these gaps, we introduce CLASE\footnote{With inspiration from Spanish ``\textit{con clase}'' (having class).} (\textbf{C}hinese \textbf{L}eg\textbf{A}lese \textbf{S}tylistic \textbf{E}valuation), a hybrid evaluation framework designed specifically for assessing stylistic fidelity in legal text generation. Our approach combines objective linguistic feature analysis with experience-guided LLM evaluation, addressing the limitations of existing methods while maintaining interpretability and reference-free operation. The framework employs contrastive learning using authentic legal documents and their stylistically-restored counterparts, enabling automatic acquisition of evaluation criteria that reflect actual professional expectations without requiring manual annotation. CLASE operates in three phases: 1) automated synthesization of contrastive training pairs; 2) training-free contrastive learning that builds experience pools; and 3) hybrid scoring combining linguistically-grounded objective measures with subjectively-informed LLM assessment. 

Our primary contributions include:

\begin{itemize}
\item \textbf{A novel hybrid evaluation architecture} that  addresses the neglected dimension of stylistic quality in legal text generation, combining objective linguistic feature analysis with experience-guided LLM evaluation to provide comprehensive stylistic assessment while maintaining interpretability and reference-free operation.

\item \textbf{A contrastive learning framework} that eliminates expensive manual annotation requirements while ensuring alignment with professional standards, employing authentic legal documents and their deliberately stylistically-degraded counterparts to automatically acquire evaluation criteria that capture actual model defects and reflect actual professional expectations.

\item \textbf{Empirical validation} demonstrating  improved correlation with human expert judgments compared to traditional metrics and pure LLM-based approaches, with interpretable, actionable natural language feedback with improvement strategies.
\end{itemize}

CLASE addresses a gap in domain-specific, stylistics-oriented evaluation, providing a scalable and transparent solution for professional stylistic assessment. The core principles can extend to other domains requiring specialized stylistic conformity, offering a general approach to stylistic evaluation in professional text generation.

\section{Related Work}

\subsection{NLG Evaluation}

NLG evaluation operates across three primary paradigms: \textit{n-gram based lexical metrics}, \textit{neural embedding approaches}, and \textit{LLM-as-a-Judge systems}. 1) Reference-based, n-gram lexical metrics such as BLEU \cite{papineni2002bleu}, ROUGE \cite{lin2004rouge}, and METEOR \cite{banerjee2005meteor} measure lexical similarity between generated text and human-written references, but these approaches face challenges in capturing semantic adequacy and stylistic appropriateness \cite{reiter2009validity,novikova2017need}. 2) Embedding-based neural metrics like BERTScore \cite{zhang2019bertscore} and MoverScore \cite{zhao2019moverscore} leverage contextual representations to improve correlation, though they measure semantic distance outside of formal dimensions \cite{schmidt2021large}. 3) LLM-as-a-Judge paradigm \cite{zheng2023judging,liu2023geval,chan2023chateval} offers reference-free assessment capabilities but introduces challenges including self-enhancement bias, positional bias, and interpretability concerns \cite{zheng_judging_2023,li_leveraging_2024}. Other reference-free evaluation methods have emerged to address reference scarcity \cite{ito_reference-free_2025}, employing techniques such as learning from human judgments through regression models \cite{rei2021references}, similarity-based approaches, and pseudo-rating methods. However, they often require substantial training and face challenges in ensuring consistent evaluation criteria.

\subsection{Legal Text Generation and Evaluation}

Early research focused on adapting general-purpose models through fine-tuning on legal corpora \cite{chalkidis2020legal}, leading to dedicated legal LLMs including LawGPT \cite{zhou2024lawgpt}, ChatLaw \cite{cui2023chatlaw}, and Lawyer-LLaMA \cite{huang2023lawyer}. These models demonstrate improvements on legal tasks compared to general models, with applications spanning document summarization \cite{deroy2023ready}, case analysis \cite{savelka2023explaining}, and legal question answering \cite{zhong2020jec}. As for benchmarking, LawBench \cite{fei2023lawbench} provides multi-dimensional assessment across memorization, understanding, and application levels. LegalEval-Q \cite{li2025legaleval} focuses on clarity, coherence, and terminology. Chinese legal benchmarks including LAiW \cite{dai2023laiw}, UCL-Bench \cite{gan2025ucl}, JuDGE \cite{su2025judge}, and CaseGen \cite{li2025casegen} offer evaluation covering various capability levels and practical applications. Gap analysis research \cite{hou2024gaps} has identified issues in LLM-generated legal analysis, highlighting the need for fine-grained evaluation, such as hybrid approaches combining automated metrics with expert human judgment \cite{guha2023legalbench,shao2025legal}.

\subsection{Style and Stylistic Evaluation}

Prevalent perspective in computational linguistics outlines ``style'' broadly as extents of formality, politeness, simplicity, personality, emotion, etc. \cite{jin_deep_2022}, focusing on style transfer (periods, genre, authors...), authorship attribution/stylistic fingerprints of LLMs \cite{bitton2025detectingstylisticfingerprintslarge}, and stylometric analysis \cite{juola2006authorship,argamon2003stylistic}. Recent advances in content-independent style embeddings address content leakage challenges. The STEL framework \cite{styleemb2023} provides standardized benchmarks for style embeddings, while StyleDistance \cite{styledistance2024} employs LLM-synthetic contrastive parallel text to create controlled stylistic variations to train separate embeddings. Our work focuses on stylistic \textit{quality} assessment, which differs from classical style transfer/analysis. We assess adherence to domain-specific conventions and professional standards in Chinese legal contexts.




\section{Method}

CLASE adopts a three-stage approach (see Figure \ref{fig:clase_overview}): \textit{contrastive pair synthesization}, \textit{training-free contrastive learning}, and \textit{hybrid scoring}. The framework requires no manual annotations while capturing both surface-level linguistic patterns and implicit stylistic norms.

\begin{figure}[ht]
  \centering
  \includegraphics[width=1\linewidth]{fig_clase.pdf}
  \caption{CLASE Overview: (I) contrastive pair synthesization from authentic legal documents; (II) training-free contrastive learning to build positive/negative example pools; (III) hybrid scoring combining objective linguistic features with experience-guided LLM evaluation.}
  \label{fig:clase_overview}
\end{figure}

\subsection{Contrastive Pair Synthesization}

We construct training exemplars from authentic Chinese judgment documents\footnote{Following professional practice, we list five sections in first-instance civil judgments: 1) header (case and party information), 2) facts (claims and findings), 3) reasoning (dispute analysis and rationale), 4) judgment (legal basis and outcome), and 5) footer (appeal information and signatures). This constitutes ``split'' in Figure \ref{fig:clase_overview}.}, \textbf{focusing on the reasoning sections} where stylistic quality most critically impacts professional acceptability. For each original text segment $t_{\text{gold}}$, we generate contrastive learning pairs through a two-stage, prompt-guided transformation: $t_{\text{reverse}} = \pi_1(t_{\text{gold}})$ and $t_{\text{restored}} = \pi_2(t_{\text{reverse}})$, where $\pi_1$ performs stylistic degradation by converting legalese to colloquial expression while preserving semantics, named entities, and topic chains, and $\pi_2$ attempts restoration from the degraded text back to legalese. 

\subsection{Training-Free Contrastive Learning}

This stage operates through structured steps to accumulate stylistic knowledge without manual annotations. Each learning step $\tau^{(i)}$ processes a contrastive pair $(t_{\text{gold}}^{(i)}, t_{\text{restored}}^{(i)})$ to extract labeled exemplars for regression.

For the $i$-th learning step $\tau^{(i)}$, we define the input as a single contrastive pair $(t_{\text{gold}}^{(i)}, t_{\text{restored}}^{(i)})$. The goal of each learning step is to identify stylistic issues and extract corresponding positive-negative exemplar pairs. Within each step, we perform guided comparison to identify a set of stylistic problems $\mathcal{I}^{(i)} = \{I_1^{(i)}, I_2^{(i)}, ..., I_{m_i}^{(i)}\}$, where each $I_j^{(i)}$ represents a specific stylistic issue discovered by comparing the gold and restored texts:

$$\mathcal{I}^{(i)} = \pi_{\text{identify}}(t_{\text{gold}}^{(i)}, t_{\text{restored}}^{(i)})$$

where $\pi_{\text{identify}}$ outputs structured issue descriptions. For each identified problem $I_j^{(i)} \in \mathcal{I}^{(i)}$, we extract a positive-negative exemplar pair $(e_{\text{pos}}^{(i,j)}, e_{\text{neg}}^{(i,j)})$ through a prompt-constrained \textbf{one-to-one correspondence} $\Theta: e_{\text{pos}}^{(i,j)} \leftrightarrow e_{\text{neg}}^{(i,j)}$, ensuring paired positions in both pools address the same stylistic aspect. The extracted exemplars are accumulated into two separate pools: positive experience pool $\mathcal{P}_{\text{pos}}$ and negative experience pool $\mathcal{P}_{\text{neg}}$. 

After completing $N$ learning steps, we perform logistic regression on the accumulated experience pools. Inspired by the method in \citet{huang_exploring_2018}'s Chinese readability assessment, we extract linguistic features $F(e) = \{f_1, f_2, ..., f_{100}\}$ from each exemplar $e$ in both pools, encompassing surface-level characteristics including character complexity, part-of-speech distributions, syntactic patterns, and discourse markers, among others\footnote{Refer to \citet{huang_exploring_2018} or CLASE repository (\url{https://github.com/rexera/CLASE}) for a comprehensive list of features.}. Features are z-score normalized to ensure commensurate scales before coefficient comparison. The regression model learns to distinguish positive from negative exemplars:

$$P(\text{positive}|F(e)) = \sigma(\mathbf{w}^T F(e) + b)$$

where $\mathbf{w}$ represents learned feature weights and $\sigma$ is the sigmoid function. This provides a feature-based scoring mechanism that generalizes from the accumulated exemplars to evaluate new texts.

\begin{algorithm}
\caption{Training-Free Contrastive Learning}
\begin{algorithmic}
\STATE \textbf{Input:} Document pairs $\{(t_{\text{gold}}^{(i)}, t_{\text{restored}}^{(i)})\}_{i=1}^{N}$
\STATE \textbf{Output:} Experience pools $\mathcal{P}_{\text{pos}}$, $\mathcal{P}_{\text{neg}}$, coefficients $\mathbf{w}$
\STATE 
\STATE $\mathcal{P}_{\text{pos}} \leftarrow \emptyset$, $\mathcal{P}_{\text{neg}} \leftarrow \emptyset$
\FOR{$i = 1$ to $N$} 
    \STATE $\mathcal{I}^{(i)} \leftarrow \pi_{\text{identify}}(t_{\text{gold}}^{(i)}, t_{\text{restored}}^{(i)})$ 
    \FOR{$j = 1$ to $|\mathcal{I}^{(i)}|$}
        \STATE $(e_{\text{pos}}^{(i,j)}, e_{\text{neg}}^{(i,j)}) \leftarrow \Phi(I_j^{(i)})$ 
        \STATE $\mathcal{P}_{\text{pos}} \leftarrow \mathcal{P}_{\text{pos}} \cup \{e_{\text{pos}}^{(i,j)}\}$, $\mathcal{P}_{\text{neg}} \leftarrow \mathcal{P}_{\text{neg}} \cup \{e_{\text{neg}}^{(i,j)}\}$ \COMMENT{$\Theta$ correspondence}
    \ENDFOR
\ENDFOR
\STATE $X \leftarrow$ [FeatureExtract($e$) for $e \in \mathcal{P}_{\text{pos}} \cup \mathcal{P}_{\text{neg}}$]
\STATE $y \leftarrow$ [1 for $e \in \mathcal{P}_{\text{pos}}$] + [0 for $e \in \mathcal{P}_{\text{neg}}$]
\STATE $\mathbf{w} \leftarrow$ LogisticRegression($X$, $y$)
\STATE \textbf{return} $\mathcal{P}_{\text{pos}}$, $\mathcal{P}_{\text{neg}}$, $\mathbf{w}$
\end{algorithmic}
\end{algorithm}



\subsection{Hybrid Scoring}

CLASE produces a final score $\Psi(t)$ through hybrid combination of objective (\textit{CLASE Obj}) and subjective (\textit{CLASE Subj}) assessments. Without reference, for each generated text $t$, it eventually offers a sigmoid-fused, $[0,1]$ score from both linguistic features and experience-guided LLM judging.

Given input text $t$, \textit{CLASE Obj} extracts linguistic features $F(t) = \{f_1, f_2, ..., f_{100}\}$ and select the top-$k$ features based on logistic regression coefficient magnitudes. The objective score is computed as:

$$\Psi_{obj}(t) = 10 \times \sigma(\mathbf{w}^T F_k(t))$$

where $F_k(t)$ represents the selected $k$ features, $\mathbf{w}$ contains learned regression weights, and the output is normalized to $[0,10]$.

\textit{CLASE Subj} evaluates seven dimensions based on legal writing practice through retrieval-augmented, experience-guided assessment. 

We define the dimension set $\mathcal{D} = \{d_1, d_2, ..., d_7\}$ with respective weights: noun usage (30\%), verb usage (30\%), adjective usage (20\%), function words (5\%), sentence coherence (5\%), sentence structure (5\%), and collocations (5\%). 

For each dimension $d \in \mathcal{D}$, 1) LLM judge $\pi$ analyzes the input text $t$ and generates $x$ queries focusing on potential stylistic issues within dimension $d$. 2) for each query, we retrieve top-$y$ negative exemplars from $\mathcal{P}_{\text{neg}}$ and obtain their corresponding positive exemplars through $\Theta$. 3) $\pi$ score the text in $[0,10]$ using these contrastive exemplar pairs as contextual guidance.

The final CLASE score combines both components through empirically-calibrated hybrid fusion. We use equal weighting (0.5 each) based on pilot studies showing optimal performance at this balance. The sigmoid transformation ensures output normalization while preserving relative rankings:

$$\Psi(t)' = 0.5 \times \Psi_{obj}(t) + 0.5 \times \Psi_{subj}(t)$$
$$\Psi(t) = \frac{1}{1 + \exp(-10 \times (\Psi(t)'/10 - 0.5))}$$

\begin{algorithm}
\caption{Hybrid Scoring}
\label{alg}
\begin{algorithmic}
\STATE \textbf{Input:} Text $t$, experience pools $\mathcal{P}_{\text{pos}}, \mathcal{P}_{\text{neg}}$, weights $\mathbf{w}$, dimension set $\mathcal{D}$, $k$, query count $x$, retrieval count $y$
\STATE \textbf{Output:} Final CLASE score $\Psi(t)$
\STATE // Objective Scoring
\STATE $F_k(t) \leftarrow$ ExtractTopKFeatures($t$, $k$)
\STATE $\Psi_{obj}(t) \leftarrow 10 \times \sigma(\mathbf{w}^T F_k(t))$
\STATE // Subjective Scoring
\STATE $\Psi_{subj}(t) \leftarrow 0$
\FOR{$d \in \mathcal{D}$}
    \STATE $\mathcal{A}_d \leftarrow \pi$.AnalyzeText($t$, $d$) \COMMENT{$\pi$ identifies potential issues in dimension $d$}
    \STATE $Q \leftarrow \pi$.GenerateQueries($\mathcal{A}_d$, $x$) 
    \STATE $\mathcal{E} \leftarrow \emptyset$
    \FOR{$q_i \in Q$}
        \STATE $N_{q_i} \leftarrow$ TopKSimilar($q_i$, $\mathcal{P}_{\text{neg}}$, $y$) \COMMENT{Retrieve $y$ similar negatives}
        \STATE $P_{q_i} \leftarrow$ GetCorresponding($N_{q_i}$, $\mathcal{P}_{\text{pos}}$) \COMMENT{Get paired positives via $\Theta$}
        \STATE $\mathcal{E} \leftarrow \mathcal{E} \cup \{(P_{q_i}, N_{q_i})\}$
    \ENDFOR
    \STATE $\Psi_{subj}(t, d) \leftarrow \pi$.Evaluate($t$, $\mathcal{E}$, $d$)
    \STATE $\Psi_{subj}(t) \leftarrow \Psi_{subj}(t) + \beta_d \cdot \Psi_{subj}(t, d)$
\ENDFOR
\STATE $\Psi(t) \leftarrow$ SigmoidFusion($\Psi_{obj}(t)$, $\Psi_{subj}(t)$)
\STATE \textbf{return} $\Psi(t)$
\end{algorithmic}
\end{algorithm}



\section{Experiments}

\subsection{Experimental Setup}

We build, train, and test CLASE on Qwen-2.5 model family \cite{qwen25_2024}. \textbf{At learning time}, we conduct experiments using 4000 Chinese civil judgment documents (learning step $N=4000$)\footnote{https://wenshu.court.gov.cn}. For the contrastive pair synthesization phase, we employ the 7B model for stylistic degradation ($\pi_1$), 32B for restoration ($\pi_2$), and 72B for experience pool construction. 

\textbf{At test time}, we sample 200 additional documents from the same data source, outside the training scope. We follow the same pipeline to generate colloquially reverse versions, then employ GPT-4o \cite{openai2024gpt4o} to create restored versions with controlled variations to simulate different legalese proficiency levels and varied emphasis on legal writing requirements: efficiency, thoroughness, structure, formality, educational\footnote{Legal writing serves as not only an instrument for the rule of law, judicial practices, and law enforcement, but also a vital medium for shaping public legal awareness. Detailed implementation is in \href{https://github.com/rexera/CLASE}{our repository}.}. 

The top-$k$ feature selection uses absolute coefficient values from L2-regularized logistic regression. Text segmentation follows jieba tokenization with character-level boundary detection for span alignment. For the subjective scoring component, we use the 72B model with a naive Chain-of-Thought (CoT) prompting \cite{wei_chain--thought_2023} as the judge model. For retrievals, we use embeddings of the 7B model with cosine similarity as the distance metric. In the main experiments, we configure the retrieval parameters as query count $x=10$ and retrieval count $y=10$, with the objective component using top $k=25$  features. All experiments are conducted on 8 NVIDIA A800-SXM4-80GB GPUs with vLLM \cite{kwon2023efficient} for model inference.

\subsection{Evaluation}

We recruit two legal domain experts to conduct evaluation for 200 restored documents across the seven aforementioned stylistic dimensions and respective weights. Each expert independently assigns scores from 0-10 for each dimension comparing gold and restored documents. Inter-annotator agreement achieves Krippendorff's alpha of 0.72 \cite{krippendorff2011computing}, indicating reliability. We evaluate system performance using Pearson correlation coefficient $r$ \cite{pearson1895note}, Spearman rank correlation $\rho$ \cite{spearman1904proof}, and Kendall's $\tau$ \cite{kendall1938new}.

\subsection{Baselines}

1) Traditional reference-based metrics include standard n-gram methods (character-level F1, BLEU, ROUGE, METEOR) and embedding-based semantic similarity (BERTScore). The F1 baseline uses character-level exact matching with precision-recall harmonic mean. 2) LLM-as-a-Judge methods employ GPT-4o-mini and Qwen-2.5-72B in both reference-based (``-ref'') and reference-free configurations, evaluating across the same seven stylistic dimensions with 0-10 scoring. 3) CLASE variants include subjective-only (CLASE-Subj), objective-only (CLASE-Obj), and hybrid fusion (CLASE-Mix).

\section{Results and Analysis}


\begin{table}[htbp]
\centering
\caption{Main Correlation Results}
\label{tab:main_weighted}
\begin{tabular}{llll}
\toprule
Method &  $r$ &  $\rho$ &  $\tau$ \\
\midrule
ROUGE & 0.4250 & 0.4807 & 0.3355 \\
BLEU & 0.4160 & 0.3383 & 0.2306 \\
F1 & 0.3907 & 0.3214 & 0.2213 \\
BERTScore & 0.3145 & 0.2010 & 0.1378 \\
METEOR & 0.3586 & 0.2949 & 0.2020 \\
GPT-4o-mini-ref & 0.3206 & 0.3061 & 0.2083 \\
Qwen2.5-72B-ref & 0.3049 & 0.2482 & 0.1808 \\
GPT-4o-mini & 0.0689 & 0.0747 & 0.0528 \\
Qwen2.5-72B & 0.2416 & 0.2336 & 0.1855 \\
CLASE-Subj & 0.3165 & 0.3063 & 0.2148 \\
CLASE-Obj & 0.7923 & 0.7568 & 0.5692 \\
\textbf{CLASE-Mix} & \textbf{0.8271} & \textbf{0.8109} & \textbf{0.6180} \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[htbp]
    \centering
    \caption{Variance and Dispersion Analysis}
    \label{tab:variance_analysis}
    \begin{tabular}{lrrr}
    \toprule
    Method & Std & Variance & CV \\
    \midrule
    \textbf{CLASE-Mix} & \textbf{2.021} & \textbf{4.086} & \textbf{0.365} \\
    Human & 1.722 & 2.965 & 0.383 \\
    Qwen2.5-72B-ref & 0.639 & 0.408 & 0.085 \\
    GPT-4o-mini-ref & 0.574 & 0.330 & 0.082 \\
    CLASE-Subj & 0.473 & 0.223 & 0.079 \\
    GPT-4o-mini & 0.473 & 0.223 & 0.062 \\
    ROUGE & 0.363 & 0.132 & 0.669 \\
    Qwen2.5-72B & 0.176 & 0.031 & 0.022 \\
    CLASE-Obj & 0.118 & 0.014 & 2.695 \\
    METEOR & 0.103 & 0.011 & 0.231 \\
    BLEU & 0.088 & 0.008 & 0.385 \\
    F1 & 0.072 & 0.005 & 0.120 \\
    BERTScore & 0.035 & 0.001 & 0.043 \\
    \bottomrule
    \end{tabular}
    \end{table}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{variance_analysis.pdf}
    \caption{Correlation analysis between evaluation methods and human judgments. Points closer to the diagonal line indicate better alignment with human evaluation.}
    \label{fig:variance_analysis}
\end{figure}

\subsection{Main Results}

\paragraph{Correlation} As shown in Table \ref{tab:main_weighted}, 1) hard metrics fail to capture stylistic variation. These metrics focus on surface-level lexical matching or semantic similarity, which is inadequate for capturing stylistic features or differentiating documents based on stylistic variation alone. 2) LLM-as-a-judge methods (soft metrics) have inherent limitations in providing fine-grained numerical scores for style. It is better suited for generating qualitative, natural language feedback and could benefit from the grounding provided by objective measures. 3) Notably, CLASE-Obj achieves strong correlation. This suggests that the contrastive learning approach successfully identifies useful linguistic features that correlate with professional legalese. When fused with CLASE-Subj, it gains incremental improvement for introducing flexible nature of LLMs. 

Figure \ref{fig:variance_analysis} visualizes this alignment through normalized scores, where the x-axis represents standardized human judgments (z-scores) and the y-axis shows standardized scores from each evaluation method. The diagonal line $y=x$ indicates perfect correlation. CLASE effectively cluster closer to the diagonal line compared to hard and soft metrics, indicating superior capability in capturing human-perceived stylistic quality. 

\paragraph{Dispersion} Table \ref{tab:variance_analysis} reveals critical evaluation characteristics through the coefficient of variation (CV), which normalizes variability by mean scores to enable fair comparison. Traditional metrics exhibit extremely low CV values, whereas LLM-as-a-Judge methods show relatively higher values yet still more ``conservative'' than human experts, indicating insufficient sensitivity to stylistic differences. CLASE-Subj performs comparably to or slightly better than LLM-as-a-judge baselines. CLASE-Mix closely matches human expert dispersion, suggesting appropriate discriminative power for stylistic assessment.



\subsection{Ablation Study}

For the subjective score (Figure \ref{fig:ablation_subjective}), performance generally improves with a larger training set size ($N$), more queries per document ($x$), and more retrieved example pairs per query ($y$). This shows that a richer context of positive and negative examples helps guide the LLM toward more accurate evaluations. For the objective score (Figure \ref{fig:ablation_objective}), we analyzed the impact of training set size ($N$) and the number of significant features ($k$). Correlation peaks at $N=4000, k=25$, indicating that a focused set of salient linguistic markers is optimal.

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{ablation_subjective_kendall.pdf}
    \caption{Ablation study on training set size and retrieval parameters for subjective scoring component, measured by Kendall's $\tau$.}
    \label{fig:ablation_subjective}
\end{figure}


\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{ablation_objective_kendall.pdf}
    \caption{Ablation study on training set size and significant feature count for objective scoring component, measured by Kendall's $\tau$.}
    \label{fig:ablation_objective}
\end{figure}


\subsection{Case Study: Stylistic Feedback}
\label{feedback}

\begin{figure}
    \centering
    \includegraphics[width=1\linewidth]{fig_case.pdf}
    \caption{Example of CLASE's natural language feedback identifying specific stylistic issues and improvement suggestions.}
    \label{fig:case}
\end{figure}

Due to preceding CoT reasoning, \textit{CLASE-Subj} provides interpretable natural language reason/feedback that identifies stylistic deficiencies and improvement directions. Figure \ref{fig:case} demonstrates how CLASE offers actionable comments about legal writing quality. The feedback examples represent issues that practitioners consistently identify when reviewing LLM-generated legal documents in actual judicial settings. Through field observations in Chinese courts, we documented frequent stylistic complaints from legal professionals, including: 

1) inappropriate lexical choices for legal basis versus factual reasoning (different formal terms must be used when citing statutes versus referencing evidentiary materials); 2) omission of conventional emphatic phrases that signal judicial authority and legal compliance (``reasonable and lawful'', ``in accordance with the law''); and 3) subtle distinctions between copular constructions that carry different degrees of formality and certainty in legal contexts.

Remarkably, CLASE detects all these precise issues without any annotation of such domain-specific knowledge in the example in Figure \ref{fig:case}. The system learns to distinguish nuanced professional conventions purely through contrastive analysis, without explicit knowledge injection. It demonstrates that sophisticated domain-specific stylistic expertise, which traditionally requires years of training, could be \textbf{potentially} acquired through automated, contrastive learning, bringing out exciting ``intelligence'' that pure numerical scores cannot offer.


\subsection{Discussion}

\paragraph{Generalizability} Current validation focuses on Chinese civil judgment reasoning sections. However, CLASE's core contrastive learning principles offer promising extensibility. The framework's reliance on natural language knowledge representation suggests potential generalization advantages—stylistic expertise encoded in natural language exhibits transferability compared to numerical features or domain-specific rules. Extension to other legal domains, broader professional writing contexts, or other languages would require domain-specific experience pool construction while preserving the underlying methodology.

\paragraph{Computational considerations} While contrastive learning requires substantial resources for pair generation and experience pool construction, the resulting system operates efficiently in deployment. Once trained, experience pools remain static\footnote{The experience pool can be updated periodically to incorporate new domain knowledge and improve performance over time.} and retrieval scale well with pre-computed embeddings and indexing. The framework's computational profile is front-loaded during development rather than inference, making it suitable for production environments where training costs amortize over extensive usage.

\paragraph{Scaling characteristics} Our ablation studies reveal complex scaling patterns that challenge simple, ``brute force'' ``scaling law'' assumptions. While performance generally improves with training set size up to $N=4000$, marginal gains diminish beyond certain thresholds. Similarly, optimal feature selection ($k=25$) suggests that excessive linguistic features introduce noise rather than improving discrimination. These findings indicate that effective deployment requires careful hyperparameter optimization rather than naive scaling strategies.

\paragraph{Interpretability and Hybrid Necessity} While \textit{CLASE-Obj} demonstrates strong performance, reliance on objective features alone is insufficient, especially when we consider evaluation as the reward signal of LLM reinforcement loop. Guiding generation policy with only objective metrics is prone to Goodhart's Law; once specific stylistic markers become explicit optimization targets, models might game the metric without genuine stylistic improvement. Furthermore, numerical feature weights offer limited explanatory power. \textit{CLASE-Subj} is imperative as it provides: 1) semantic grounding to prevent adversarial overfitting to surface features; and 2) actionable natural language feedback (as shown in Section \ref{feedback}), which is essential for human-in-the-loop workflows. 

\section{Conclusion}

We introduce CLASE, a hybrid evaluation framework that addresses the under-explored challenge of stylistic quality assessment in Chinese legal text generation. By combining objective linguistic feature analysis with experience-guided LLM evaluation, our approach provides a reference-free solution that captures both surface-level patterns and implicit professional conventions in Chinese legal writing.

The key insight underlying CLASE is that professional stylistic expertise can be acquired through contrastive analysis rather than explicit annotation. Our training-free contrastive learning framework successfully extracts meaningful stylistic knowledge from authentic legal documents and their deliberately degraded counterparts, eliminating the need for expensive manual score annotations while maintaining alignment with professional standards.

Experimental results on 200 Chinese legal documents demonstrate that CLASE achieves higher correlation with human expert judgments compared to traditional metrics and pure LLM-based approaches. The framework not only provides quantitative scores but also generates interpretable natural language feedback that identifies specific stylistic deficiencies and improvement directions—capabilities that numerical metrics cannot offer.

The work makes three primary contributions to the intersection of natural language evaluation and legal AI: establishing a hybrid architecture that balances objectivity with nuanced judgment, demonstrating that sophisticated domain-specific conventions can emerge from automated contrastive analysis, and providing a scalable framework that extends beyond Chinese legal text to other domains requiring specialized stylistic conformity.

While current validation focuses on civil judgment documents within a specific model family, the core principles of contrastive stylistic learning offer a general approach to professional text evaluation. Future work should explore cross-domain generalization, computational efficiency optimization, and deeper integration of legal writing principles to enhance both performance and interpretability. As legal AI systems become increasingly sophisticated in generating factually accurate content, frameworks like CLASE become essential for ensuring that automated text generation meets the professional standards expected in legal practice.

\section{Ethical Considerations}

This research primarily involves publicly available legal documents (Chinese civil judgments) and standard large language models. The datasets do not contain private or sensitive individual information that is not already part of the public record. The proposed evaluation framework aims to improve the professional quality of automated legal writing. We anticipate no direct negative societal impacts. However, users should be aware that automated evaluation tools, including CLASE, should serve as assistants to, rather than replacements for, human legal professionals.

\section{Limitations}

We acknowledge several limitations in the current work. First, our experiments are restricted to Chinese civil judgments. While we believe the contrastive learning methodology is transferrable, the specific implementation of the objective component (using Chinese linguistic features) is language-dependent and would require adaptation for other languages. Second, the contrastive pair synthesis relies on the assumption that the degradation model alters style without corrupting semantics. While our strong correlation results suggest the validity of the generated data for training, we did not perform a large-scale manual audit of the synthetic pairs. Third, compared to lightweight metrics like BLEU, CLASE involves a multi-stage pipeline which demands higher computational resources, potentially limiting immediate adoption in low-latency applications.

While CLASE-Obj demonstrates strong empirical performance, the underlying mechanisms by which logistic regression weights capture professional stylistic preferences remain opaque. CLASE-Subj exhibits sophisticated pattern recognition capabilities but lacks deep causal understanding. It identifies specific stylistic deficiencies without comprehending the underlying professional rationales or historical evolution of legal writing conventions. This ``knowing-what but not knowing-why'' limitation restricts the framework's ability to provide educational insights or adapt to evolving professional standards. Future work should explore incorporating explicit legal writing principles to achieve more comprehensive stylistic expertise.

\section{Acknowledgements}
We thank the reviewers for their human touch, dedication, and insightful feedback. This work was supported by the Natural Language Processing Lab at Tsinghua University (TsinghuaNLP).

\bibliographystyle{lrec2026-natbib}
\bibliography{lrec2026-example}

\end{document}

%%% Local Variables:
%%% mode: latex
%%% TeX-master: t
%%% End:
