import os
import json
import time
import random
from openai import OpenAI
from datetime import datetime
import shutil
from concurrent.futures import ThreadPoolExecutor, as_completed
import tqdm
from tqdm import tqdm
from dotenv import load_dotenv

def progressive_comparative_learning_parallel(reason_file_path, restored_file_path, max_samples=100, verbose=False, output_prefix=None, resume_from_step=None):
    """
    Extracts precise, typical, and concise positive and negative examples from document pairs.
    
    Args:
        reason_file_path (str): Original legal document file path
        restored_file_path (str): Restored document file path  
        max_samples (int): Maximum number of samples to process, default 100
        verbose (bool): Whether to print detailed debug info, default False
        output_prefix (str): Output file prefix, auto-generated by default
        resume_from_step (int): Resume from specified step, default from beginning
    
    Returns:
        tuple: (progress_info, examples_jsonl_path)
    """
    load_dotenv()
    
    client = OpenAI(
        base_url=os.getenv('BASE_URL'),
        api_key=os.getenv('OPENAI_API_KEY'),
    )
    MODEL = os.getenv('MODEL', 'gpt-4o-mini')
    
    if output_prefix is None:
        output_prefix = f"clase_exp/model_output"
    
    examples_jsonl = f"{output_prefix}/examples.jsonl"
    progress_file = f"{output_prefix}/progress.json"
    
    def save_progress(progress, file_path):
        os.makedirs(os.path.dirname(file_path), exist_ok=True)
        with open(file_path, 'w', encoding='utf-8') as f:
            json.dump(progress, f, ensure_ascii=False, indent=2)
    
    existing_progress = []
    if os.path.exists(progress_file):
        with open(progress_file, 'r', encoding='utf-8') as f:
            existing_progress = json.load(f)
    
    progress = existing_progress
    
    def debug_print(message, level=0):
        if not verbose:
            return
        indent = "  " * level
        timestamp = datetime.now().strftime("%H:%M:%S")
        print(f"[{timestamp}] {indent}{message}")
    
    def call_llm_with_retry(prompt, model=MODEL, response_format=None):
        messages = [
            {"role": "system", "content": "You are an expert in extracting precise, typical, and concise examples of stylistic features in Chinese legal documents. Always respond in JSON format."},
            {"role": "user", "content": prompt}
        ]
        
        kwargs = {
            "model": model,
            "messages": messages,
        }
        
        if response_format:
            kwargs["response_format"] = response_format
        
        completion = client.chat.completions.create(**kwargs)
        response_text = completion.choices[0].message.content
        
        if response_format:
            return json.loads(response_text)
        
        return response_text
    
    def extract_examples(gold_text, restored_text):
        prompt = f"""
        Extract positive and negative stylistic examples from the following legal documents. Examples must be precise, typical, and concise, fully reflecting the model's actual deficiencies and underperformance.

        Original document (positive example source):
        {gold_text}
        
        Restored document (negative example source):
        {restored_text}
        
        Extraction requirements:
        - Extract several pairs of corresponding positive and negative examples
        - Positive examples: Extract typical excellent stylistic examples from original document (e.g., formal terminology, rigorous sentence structure)
        - Negative examples: Extract corresponding typical deficient stylistic examples from restored document (e.g., informal expressions, loose sentence structure)
        - Each example should be a short text fragment
        - Positive and negative examples must correspond one-to-one with equal quantity
        - Each positive and negative field must be string type
        
        Return strictly in the following JSON format, do not add any extra content:
        {{"examples": [{{"positive": "positive example text", "negative": "negative example text"}}, {{"positive": "positive example text", "negative": "negative example text"}}]}}
        """
        
        response_format = {"type": "json_object"}
        result = call_llm_with_retry(prompt, response_format=response_format)
        
        return result.get('examples', [])
    
    def load_data():
        with open(reason_file_path, 'r', encoding='utf-8') as f:
            reason_data = json.load(f)
        
        restored_data = []
        with open(restored_file_path, 'r', encoding='utf-8') as f:
            for line in f:
                if line.strip():
                    restored_data.append(json.loads(line))
        
        return reason_data, restored_data
    
    def validate_data_alignment(reason_data, restored_data):
        aligned_pairs = []
        reason_dict = {item['index']: item for item in reason_data if 'index' in item}
        for restored in restored_data:
            idx = restored.get('index')
            if idx in reason_dict:
                aligned_pairs.append({
                    'index': idx,
                    'reason_data': reason_dict[idx],
                    'restored_data': restored
                })
        return aligned_pairs
    
    print(f"Starting example extraction - processing first {max_samples} samples")
    print(f"Examples file: {examples_jsonl}")
    
    reason_data, restored_data = load_data()
    aligned_pairs = validate_data_alignment(reason_data, restored_data)
    
    max_available = min(len(aligned_pairs), max_samples)
    aligned_pairs = aligned_pairs[:max_available]
    
    if resume_from_step is None:
        if progress:
            completed_steps = [p['step'] for p in progress if p.get('successful', False)]
            resume_from_step = max(completed_steps) + 1 if completed_steps else 1
        else:
            resume_from_step = 1
    
    batch_size = 10
    start_index = max(0, (resume_from_step - 1) // batch_size * batch_size)
    
    batch_size = 10
    batches = [aligned_pairs[i:i + batch_size] for i in range(start_index, len(aligned_pairs), batch_size)]
    
    step = resume_from_step
    total_samples = len(aligned_pairs)
    
    pbar = tqdm(total=total_samples, initial=(resume_from_step - 1), desc="Processing samples")
    
    all_results = []
    
    try:
        for batch_num, batch in enumerate(batches, 1):
            print(f"Processing batch {batch_num} of {len(batches)}")
            
            with ThreadPoolExecutor(max_workers=10) as executor:
                future_to_info = {executor.submit(extract_examples, pair['reason_data'].get('reason', ''), pair['restored_data'].get('restored', '')): (step + batch_step, pair['index']) for batch_step, pair in enumerate(batch) if pair['reason_data'].get('reason', '') and pair['restored_data'].get('restored', '')}
                
                for future in as_completed(future_to_info):
                    local_step, idx = future_to_info[future]
                    try:
                        pairs = future.result()
                        all_results.append({"step": local_step, "pair": pairs})
                        progress.append({"step": local_step, "index": idx, "successful": True})
                    except Exception as e:
                        print(f"Error in step {local_step}: {e}")
                        progress.append({"step": local_step, "index": idx, "successful": False})
                    pbar.update(1)
            
            save_progress(progress, progress_file)
            step += len(batch)
    except KeyboardInterrupt:
        print("\nProcessing interrupted. Saving progress...")
        save_progress(progress, progress_file)
        print("Progress saved. You can resume by running again.")
    finally:
        pbar.close()
    
    if all_results:
        print("Sorting results by step number...")
        all_results.sort(key=lambda x: x['step'])
        
        os.makedirs(os.path.dirname(examples_jsonl), exist_ok=True)
        with open(examples_jsonl, 'w', encoding='utf-8') as f:
            for result in all_results:
                json_line = json.dumps(result, ensure_ascii=False)
                f.write(json_line + '\n')
        print(f"Sorted results written to {examples_jsonl}")
    
    return progress, examples_jsonl

if __name__ == "__main__":
    reason_file = "data/reason_documents.json"
    restored_file = "data/restored_documents.jsonl"
    
    progress, examples_jsonl = progressive_comparative_learning_parallel(
        reason_file, 
        restored_file, 
        max_samples=1000, 
        verbose=True
    )
    
    print(f"Processing completed! Examples: {examples_jsonl}") 